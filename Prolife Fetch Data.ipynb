{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pickle\n",
    "import json\n",
    "def load_keys(path):\n",
    "    \"\"\"Loads your Twitter authentication keys from a file on disk.\n",
    "    \n",
    "    Args:\n",
    "        path (str): The path to your key file.  The file should\n",
    "          be in JSON format and look like this (but filled in):\n",
    "            {\n",
    "                \"consumer_key\": \"<your Consumer Key here>\",\n",
    "                \"consumer_secret\":  \"<your Consumer Secret here>\",\n",
    "                \"access_token\": \"<your Access Token here>\",\n",
    "                \"access_token_secret\": \"<your Access Token Secret here>\"\n",
    "            }\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary mapping key names (like \"consumer_key\") to\n",
    "          key values.\"\"\"\n",
    "    # Loading your keys from keys.json (which you should have filled\n",
    "    # in in question 1):\n",
    "    with open(\"keys.json\") as f:\n",
    "        keys = json.load(f)\n",
    "        \n",
    "    mapping = {}     \n",
    "    mapping[\"consumer_key\"] = keys[\"consumer_key\"]\n",
    "    mapping[\"consumer_secret\"] = keys[\"consumer_secret\"]\n",
    "    mapping[\"access_token\"] = keys[\"access_token\"]\n",
    "    mapping[\"access_token_secret\"] = keys[\"access_token_secret\"]\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from twython import Twython # pip install twython\n",
    "import time # standard lib\n",
    "\n",
    "credentials = load_keys(\"keys.json\")\n",
    "twitter = Twython(credentials[\"consumer_key\"],credentials[\"consumer_secret\"],\n",
    "                  credentials[\"access_token\"],credentials[\"access_token_secret\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fetchData(user):\n",
    "    user_timeline = twitter.get_user_timeline(screen_name=user,count=1)\n",
    "    len(user_timeline)\n",
    "    lis = [user_timeline[0][\"id\"]] ## this is the latest starting tweet id\n",
    "    #i = 0\n",
    "    with open(\"./prolife/{}.txt\".format(user), \"w\", encoding='utf-8', errors='ignore') as f:\n",
    "        try:\n",
    "            for i in range(0, 16): ## iterate through all tweets\n",
    "            ## tweet extract method with the last list item as the max_id\n",
    "                user_timeline = twitter.get_user_timeline(screen_name=user,\n",
    "                count=200, include_retweets=True, max_id=lis[-1])\n",
    "                time.sleep(300) ## 5 minute rest between api calls\n",
    "  \n",
    "                for tweet in user_timeline:\n",
    "                    #i += 1\n",
    "                    f.write(tweet['text']) ## print the tweet\n",
    "                    lis.append(tweet['id']) ## append tweet id's\n",
    "        except:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_recent_tweets_by_user(user_account_name, keys):\n",
    "    \"\"\"Downloads tweets by one Twitter user.\n",
    "\n",
    "    Args:\n",
    "        user_account_name (str): The name of the Twitter account\n",
    "          whose tweets will be downloaded.\n",
    "        keys (dict): A Python dictionary with Twitter authentication\n",
    "          keys (strings), like this (but filled in):\n",
    "            {\n",
    "                \"consumer_key\": \"<your Consumer Key here>\",\n",
    "                \"consumer_secret\":  \"<your Consumer Secret here>\",\n",
    "                \"access_token\": \"<your Access Token here>\",\n",
    "                \"access_token_secret\": \"<your Access Token Secret here>\"\n",
    "            }\n",
    "\n",
    "    Returns:\n",
    "        list: A list of Status objects, each representing one tweet.\"\"\"\n",
    "    auth = tweepy.OAuthHandler(keys[\"consumer_key\"], keys[\"consumer_secret\"])\n",
    "    auth.set_access_token(keys[\"access_token\"], keys[\"access_token_secret\"])\n",
    "    api = tweepy.API(auth)\n",
    "\n",
    "    # Getting as many recent tweets by user_account_name as Twitter will let us have:\n",
    "    tweets_list = list(tweepy.Cursor(api.user_timeline, id=user_account_name).items())\n",
    "    return tweets_list\n",
    "\n",
    "def save_tweets_txt(tweets, path):\n",
    "    \"\"\"Saves a list of tweets to a file in the local filesystem.\n",
    "    \n",
    "    This function makes no guarantee about the format of the saved\n",
    "    tweets, **except** that calling load_tweets(path) after\n",
    "    save_tweets(tweets, path) will produce the same list of tweets\n",
    "    and that only the file at the given path is used to store the\n",
    "    tweets.  (That means you can implement this function however\n",
    "    you want, as long as saving and loading works!)\n",
    "\n",
    "    Args:\n",
    "        tweets (list): A list of tweet objects (of type Status) to\n",
    "          be saved.\n",
    "        path (str): The place where the tweets will be saved.\n",
    "\n",
    "    Returns:\n",
    "        None\"\"\"\n",
    "    # Saving the tweets to a file as \"txt\" objects:\n",
    "    with open(path, \"w\", encoding='utf-8', errors='ignore') as f:\n",
    "        for tweet in tweets:\n",
    "            f.write(tweet.text) ## print the tweet\n",
    "\n",
    "def save_tweets_pkl(tweets, path):\n",
    "    \"\"\"Saves a list of tweets to a file in the local filesystem.\n",
    "    \n",
    "    This function makes no guarantee about the format of the saved\n",
    "    tweets, **except** that calling load_tweets(path) after\n",
    "    save_tweets(tweets, path) will produce the same list of tweets\n",
    "    and that only the file at the given path is used to store the\n",
    "    tweets.  (That means you can implement this function however\n",
    "    you want, as long as saving and loading works!)\n",
    "\n",
    "    Args:\n",
    "        tweets (list): A list of tweet objects (of type Status) to\n",
    "          be saved.\n",
    "        path (str): The place where the tweets will be saved.\n",
    "\n",
    "    Returns:\n",
    "        None\"\"\"\n",
    "    # Saving the tweets to a file as \"pickled\" objects:\n",
    "    with open(path, \"wb\") as outfile:\n",
    "        pickle.dump(tweets, outfile)\n",
    "        \n",
    "def load_tweets(path):\n",
    "    \"\"\"Loads tweets that have previously been saved.\n",
    "    \n",
    "    Calling load_tweets(path) after save_tweets(tweets, path)\n",
    "    will produce the same list of tweets.\n",
    "    \n",
    "    Args:\n",
    "        path (str): The place where the tweets were be saved.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of Status objects, each representing one tweet.\"\"\"\n",
    "    with open(path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "def fetch_tweets(user_account_name, keys_path):\n",
    "    \"\"\"Get recent tweets from one user, loading from a disk cache if available.\n",
    "    \n",
    "    The first time you call this function, it will download tweets by\n",
    "    a user.  Subsequent calls will not re-download the tweets; instead\n",
    "    they'll load the tweets from a save file in your local filesystem.\n",
    "    All this is done using the functions you defined in the previous cell.\n",
    "    This has benefits and drawbacks that often appear when you cache data:\n",
    "    \n",
    "    +: Using this function will prevent extraneous usage of the Twitter API.\n",
    "    +: You will get your data much faster after the first time it's called.\n",
    "    -: If you really want to re-download the tweets (say, to get newer ones,\n",
    "       or because you screwed up something in the previous cell and your\n",
    "       tweets aren't what you wanted), you'll have to find the save file\n",
    "       (which will look like <something>_recent_tweets.pkl) and delete it.\n",
    "    \n",
    "    Args:\n",
    "        user_account_name (str): The Twitter handle of a user, without the @.\n",
    "        keys_path (str): The path to a JSON keys file in your filesystem.\n",
    "    \"\"\"\n",
    "    \n",
    "    save_path_txt = \"./prolife/{}.txt\".format(user_account_name)\n",
    "    save_path_pkl = \"{}_recent_tweets.pkl\".format(user_account_name)\n",
    "    \n",
    "    from pathlib import Path\n",
    "    if not Path(save_path_pkl).is_file():\n",
    "        keys = load_keys(keys_path)\n",
    "        tweets = download_recent_tweets_by_user(user_account_name, keys)\n",
    "        save_tweets_txt(tweets, save_path_txt)\n",
    "        save_tweets_pkl(tweets, save_path_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users = [\n",
    "\"AbortionGroup\",\n",
    "\"prolifecampaign\",\n",
    "\"AcpAbortion\",\n",
    "\"RosaryMovement\",\n",
    "\"recallabortion\",\n",
    "\"AbortionGroup\",\n",
    "\"ExposeAbortion\",\n",
    "\"40daysforlife\",\n",
    "\"S2EAS_\",\n",
    "\"LifeNewsHQ\",\n",
    "\"AbolitionAHA\",\n",
    "\"AntiAbrtionGang\",\n",
    "\"FightAbortion\",\n",
    "\"ProLifeBlogs\",\n",
    "\"Bound4LIFE\",\n",
    "\"HumanCoalition\",\n",
    "\"LdnAntiAbortion\",\n",
    "\"PLAM_org\",\n",
    "\"ProLifeLSU\",\n",
    "\"NeverAbortion\",\n",
    "\"operationrescue\",\n",
    "\"KeepLifeLegal\",\n",
    "\"ProLifePolitics\",\n",
    "\"ProLifeYouth\",\n",
    "\"CA_ProLife\"]\n",
    "\n",
    "users = [\"ProLifeYouth\",\n",
    "\"CA_ProLife\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: ProLifeYouth in process\n",
      "user: CA_ProLife in process\n"
     ]
    }
   ],
   "source": [
    "for user in users:\n",
    "    print(\"user:\", user, \"in process\")\n",
    "    time.sleep(400) ## 5 minute rest between api calls\n",
    "    fetch_tweets(user, \"keys.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for user in users:\n",
    "    print(\"user:\", user, \"in process\")\n",
    "    fetchData(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
